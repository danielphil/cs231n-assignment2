{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_pipeline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115306114913974329"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "f = { \"param_a\": [1, 2, 3], \"param_b\": [True, False]}\n",
    "hash(json.dumps(f, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h1_strides': 2, 'max_pool': True}\n",
      "........."
     ]
    }
   ],
   "source": [
    "def sandbox_model(X,y,is_training, params):\n",
    "    h1_strides = params['h1_strides']\n",
    "    use_max_pool = params['max_pool']\n",
    "        \n",
    "    # 32 layer conv2d with 3x3 filters\n",
    "    h1 = tf.layers.conv2d(X, 32, [3, 3], strides=(h1_strides, h1_strides), activation=tf.nn.relu)\n",
    "    \n",
    "    # batch normalization\n",
    "    \n",
    "    bn = tf.layers.batch_normalization(h1, axis=3, training=is_training)\n",
    "\n",
    "    # max pooling 2x2 with stride 2\n",
    "    # max_pool shape (?, 15, 15, 32)\n",
    "    if use_max_pool:\n",
    "        max_pool = tf.layers.max_pooling2d(bn, [2, 2], [2, 2])\n",
    "    else:\n",
    "        max_pool = bn\n",
    "        \n",
    "    # affine layer with 1024 output units and relu\n",
    "    inputs = max_pool.shape[1] * max_pool.shape[2] * max_pool.shape[3]\n",
    "    max_pool_flat = tf.reshape(max_pool,[-1,inputs])\n",
    "    h2 = tf.layers.dense(max_pool_flat, 1024, activation=tf.nn.relu)\n",
    "    \n",
    "    # affine layer 2 with 10 outputs  \n",
    "    y_out = tf.layers.dense(h2, 10, activation=None)\n",
    "\n",
    "    return y_out\n",
    "\n",
    "options = {\n",
    "    \"h1_strides\": [1, 2, 3, 4],\n",
    "    \"max_pool\": [True, False],\n",
    "    \"dense_size\": [512, 1024, 2048, 4096]\n",
    "}\n",
    "\n",
    "for option_dict in tf_pipeline.random_search_params(options):\n",
    "    print(option_dict)\n",
    "    tf_pipeline.train_and_plot(\n",
    "        lambda X, y, is_training: sandbox_model(X,y,is_training, option_dict),\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
